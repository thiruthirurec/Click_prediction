{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee353e42-ff58-4955-9608-12865bd0950e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Default notebook\n",
    "\n",
    "This default notebook is executed using Databricks Workflows as defined in resources/adflow_click_analaysis_and_modeling_job.yml."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.xgboost\n",
    "import mlflow.pyfunc\n",
    "\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.functions import col, hour, to_timestamp, lit, udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.api.types import CategoricalDtype\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK, SparkTrials\n",
    "\n",
    "import joblib\n",
    "import logging\n",
    "import sys\n",
    "import json\n",
    "import traceback\n",
    "from typing import List, Dict, Any, Tuple\n",
    "import yaml\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "# Import libraries\n",
    "%run reference\n",
    "print(ENV_VARS)\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spark() -> SparkSession:\n",
    "    try:\n",
    "        from databricks.connect import DatabricksSession\n",
    "        return DatabricksSession.builder.getOrCreate()\n",
    "    except ImportError:\n",
    "        return SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_user_email():\n",
    "    try:\n",
    "        return dbutils.notebook.entry_point.getDbutils().notebook().getContext().tags().apply('user')\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to get user email: {str(e)}\")\n",
    "        return \"unknown_user@fluentco.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_query(spark: SparkSession, run_id):\n",
    "    # Create schemas if they don't exist\n",
    "    spark.sql(f\"CREATE SCHEMA IF NOT EXISTS centraldata_{ENV_VARS['ENV']}.datascience_model_deployment\")\n",
    "    \n",
    "    # Common part of the query for both environments\n",
    "    common_query = \"\"\"\n",
    "    SELECT\n",
    "        timestamp,\n",
    "        COALESCE(click, 0) AS click,\n",
    "        COALESCE(profile_gender, gender) AS gender,\n",
    "        campdata_campaignid AS campaign_id,\n",
    "        trafsrc_sourceid AS traffic_source_id,\n",
    "        trafsrc_trafficpartnerid AS traffic_partner_id,\n",
    "        device_osname AS os_name,\n",
    "        exact_age AS age,\n",
    "        est_household_income AS household_income,\n",
    "        campdata_flowimpressionposition AS position,\n",
    "        date_part('HOUR', timestamp) AS hour\n",
    "    FROM\n",
    "        centraldata_prod.glue.adflow_mltesting_event_v2 a\n",
    "    LEFT JOIN\n",
    "        centraldata_prod.transunion.gold_email_details b\n",
    "    ON\n",
    "        a.profile_emailsha256 = b.sha256_email_address\n",
    "    WHERE\n",
    "        timestamp > date_add(DAY, -22, now()) \n",
    "    AND\n",
    "        (campdata_flowimpressionposition <= 4 OR campdata_flowimpressionposition IS NULL)\n",
    "    AND trafsrc_trafficpartnerid != '1ac29301-78ac-46c6-a79d-0632ad980e3a'\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Function to check if table exists\n",
    "    def check_table_exists(env):\n",
    "        table_exists = spark.sql(f\"\"\"\n",
    "            SHOW TABLES IN centraldata_{env}.datascience_model_deployment\n",
    "            LIKE 'adflow_click_training_data_version_2'\n",
    "        \"\"\").count() > 0\n",
    "        return table_exists\n",
    "\n",
    "\n",
    "    # Function to execute and log query for each environment\n",
    "    def execute_and_log_query(env, query):\n",
    "        table_name = f\"centraldata_{env}.datascience_model_deployment.adflow_click_training_data_version_2\"\n",
    "\n",
    "        # Check if the table exists, if not create it\n",
    "        if not check_table_exists(env):\n",
    "            spark.sql(f\"CREATE TABLE {table_name} AS {common_query} LIMIT 0\")\n",
    "            print(f\"Table '{table_name}' created.\")\n",
    "        else:\n",
    "            print(f\"Table '{table_name}' already exists.\")\n",
    "\n",
    "        # Count rows before insertion\n",
    "        before_count_df = spark.sql(f\"SELECT COUNT(*) as row_count FROM {table_name}\")\n",
    "        before_count = before_count_df.collect()[0]['row_count']\n",
    "\n",
    "        mlflow.log_metric(f\"rows_before_insert_{env}\", before_count)\n",
    "\n",
    "        # Insert the data into the table\n",
    "        spark.sql(query)\n",
    "\n",
    "        # Count rows after insertion\n",
    "        after_count_df = spark.sql(f\"SELECT COUNT(*) as row_count FROM {table_name}\")\n",
    "        after_count = after_count_df.collect()[0]['row_count']\n",
    "\n",
    "        mlflow.log_metric(f\"rows_after_insert_{env}\", after_count)\n",
    "        \n",
    "        # Calculate the number of rows inserted\n",
    "        rows_inserted = after_count - before_count\n",
    "\n",
    "        # Log results to MLflow\n",
    "        mlflow.log_metric(f\"rows_inserted_{env}\", rows_inserted)\n",
    "        mlflow.log_param(f\"data_query_{env}\", query.strip())\n",
    "        print(f\"Query executed and {rows_inserted} rows inserted in {table_name}\")\n",
    "\n",
    "    # Query for the sandbox/dev schema as per env\n",
    "    insert_query = f\"\"\"\n",
    "    INSERT INTO centraldata_{ENV_VARS['ENV']}.datascience_model_deployment.adflow_click_training_data_version_2\n",
    "    {common_query}\n",
    "    AND timestamp > (SELECT COALESCE(MAX(timestamp), date_add(DAY, -22, now())) \n",
    "                     FROM centraldata_{ENV_VARS['ENV']}.datascience_model_deployment.adflow_click_training_data_version_2)\n",
    "    \"\"\"\n",
    "    execute_and_log_query(ENV_VARS['ENV'], insert_query)\n",
    "\n",
    "\n",
    "def main():\n",
    "    spark = get_spark()\n",
    "\n",
    "    # Get the current user's email to set the experiment name\n",
    "    user_email = get_current_user_email()\n",
    "\n",
    "    experiment_name = f\"/Users/{user_email}/xgboost_{ENV_VARS['ENV']}\"\n",
    "\n",
    "    # # Set MLflow experiment\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "\n",
    "    # Start MLflow run with the model name as the run name\n",
    "    with mlflow.start_run(run_name=f\"xgboost_{ENV_VARS['ENV']}_run\") as run:\n",
    "        run_id = run.info.run_id\n",
    "        execute_query(spark, run_id)\n",
    "\n",
    "        # Save the run_id for use in the second notebook\n",
    "        dbutils.jobs.taskValues.set(\"run_id\", run_id)\n",
    "        print(f\"Run ID saved: {run_id}\")\n",
    "\n",
    "    return run_id\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "data_ingestion",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
