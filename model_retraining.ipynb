{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9198e987-5606-403d-9f6d-8f14e6a4017f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.xgboost\n",
    "import mlflow.pyfunc\n",
    "\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.functions import col, hour, to_timestamp, lit, udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.api.types import CategoricalDtype\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK, SparkTrials\n",
    "\n",
    "import joblib\n",
    "import logging\n",
    "import sys\n",
    "import json\n",
    "import traceback\n",
    "from typing import List, Dict, Any, Tuple\n",
    "import yaml\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "# Import libraries\n",
    "%run reference\n",
    "print(ENV_VARS, MODELS_NAME)\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "RUN_ID = dbutils.jobs.taskValues.get(taskKey=\"data_ingestion_task\", key=\"run_id\")\n",
    "print(f\"RUN_ID: {RUN_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3fc19dba-61fd-4a89-8f8c-24fee63bfb14",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def initialize_spark():\n",
    "    try:\n",
    "        spark = SparkSession.builder \\\n",
    "            .appName(\"ClickModel\") \\\n",
    "            .config(\"spark.executor.memory\", \"4g\") \\\n",
    "            .config(\"spark.driver.memory\", \"4g\") \\\n",
    "            .getOrCreate()\n",
    "        return spark\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to initialize Spark session: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def get_current_user_email():\n",
    "    try:\n",
    "        return dbutils.notebook.entry_point.getDbutils().notebook().getContext().tags().apply('user')\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to get user email: {str(e)}\")\n",
    "        return \"unknown_user@fluentco.com\"\n",
    "\n",
    "\n",
    "def get_params_from_experiment(experiment_name):\n",
    "    try:\n",
    "        # Get experiment ID\n",
    "        experiment_id = mlflow.get_experiment_by_name(experiment_name).experiment_id\n",
    "\n",
    "        # Get runs for the experiment, sorted by start time in descending order\n",
    "        runs_ = mlflow.search_runs(experiment_ids=[experiment_id])\n",
    "\n",
    "        runs = runs_.sort_values(by='end_time', ascending=False).reset_index().drop('index', axis=1)\n",
    "\n",
    "        # Access log parameter values of the latest run\n",
    "        if runs.shape[0] > 0:\n",
    "            hyper_params_ = runs.iloc[0][f'params.{MODELS_NAME[\"MODEL1\"]}_hyperparams']\n",
    "            hyper_params = eval(hyper_params_)\n",
    "        else:\n",
    "            print(\"No runs found.\")\n",
    "\n",
    "    except:\n",
    "        hyper_params = None\n",
    "\n",
    "    return hyper_params\n",
    "\n",
    "\n",
    "def load_data(spark):\n",
    "    try:\n",
    "        query = \"\"\"\n",
    "        SELECT * \n",
    "        FROM centraldata_{}.datascience_model_deployment.adflow_click_training_data_version_2\n",
    "        WHERE timestamp > date_add(DAY, -22, now())\n",
    "        \"\"\".format(ENV_VARS['ENV'])\n",
    "        df_spark = spark.sql(query)\n",
    "        return df_spark\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load data: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def test_train_split(df_spark, train_test_ratio=0.8):\n",
    "    try:\n",
    "        train_test_splits = df_spark.randomSplit([train_test_ratio, 1 - train_test_ratio], seed=1234)\n",
    "        train_data = train_test_splits[0]\n",
    "        test_data = train_test_splits[1]\n",
    "        return train_data.toPandas(), test_data.toPandas()\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in train-test split: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def get_encodings(df, features):\n",
    "    le_dict = {}\n",
    "    for col in features:\n",
    "        if col in df.columns:\n",
    "            le_dict[col] = df[col].astype(\"category\").cat.categories\n",
    "    return le_dict\n",
    "\n",
    "\n",
    "def encode_features(df, le_dict):\n",
    "    for col in le_dict.keys():\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(CategoricalDtype(categories=le_dict[col]))\n",
    "        else:\n",
    "            logger.warning(f\"Column {col} not found in DataFrame\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def apply_feature_types(df: pd.DataFrame, features_types: Dict[str, type]) -> pd.DataFrame:\n",
    "    for feature, dtype in features_types.items():\n",
    "        if dtype == str:\n",
    "            df[feature] = df[feature].astype('category')  # Ensure it's retained as a categorical type\n",
    "        else:\n",
    "            df[feature] = df[feature].astype(dtype)\n",
    "    return df\n",
    "\n",
    "\n",
    "def convert_integers_with_missing_to_float(df: pd.DataFrame, features_types: Dict[str, type]) -> pd.DataFrame:\n",
    "    for feature, dtype in features_types.items():\n",
    "        if pd.api.types.is_integer_dtype(df[feature]):\n",
    "            # Check if the column is an integer and has null values\n",
    "            if df[feature].isnull().any():\n",
    "                df[feature] = df[feature].astype('float64')\n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_os(os_name: str) -> str:\n",
    "    if os_name in [\"Android\", \"iOS\", \"Windows\", \"Mac\"]:\n",
    "        return os_name\n",
    "    return \"Other\"\n",
    "\n",
    "\n",
    "def clean_gender(gender: str) -> str:\n",
    "    gender = gender.lower()\n",
    "    if gender in [\"f\", \"female\"]:\n",
    "        return \"F\"\n",
    "    if gender in [\"m\", \"male\"]:\n",
    "        return \"M\"\n",
    "    return None\n",
    "\n",
    "\n",
    "def clean_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df[\"os_name\"] = df[\"os_name\"].apply(clean_os)\n",
    "    df[\"gender\"] = df[\"gender\"].apply(clean_gender)\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_robust_signature(X_test: pd.DataFrame, model: XGBClassifier, n_examples: int = 5) -> tuple:\n",
    "    try:\n",
    "        input_example = X_test.sample(n=n_examples, random_state=42).copy()\n",
    "        for col in input_example.columns:\n",
    "            input_example.loc[input_example.index[0], col] = np.nan\n",
    "        \n",
    "        signature = mlflow.models.infer_signature(X_test, model.predict_proba(X_test))\n",
    "        return signature, input_example\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in create_robust_signature: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_os_udf = udf(clean_os, StringType())\n",
    "clean_gender_udf = udf(clean_gender, StringType())\n",
    "\n",
    "\n",
    "def clean_data(df_spark):\n",
    "    df_spark = df_spark.withColumn(\"os_name\", clean_os_udf(df_spark[\"os_name\"]))\n",
    "    df_spark = df_spark.withColumn(\"gender\", clean_gender_udf(df_spark[\"gender\"]))\n",
    "    return df_spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_model_and_metrics(model: XGBClassifier, X_train: pd.DataFrame, y_train: pd.Series, X_test: pd.DataFrame, y_test: pd.Series, \n",
    "                          hyperparams: Dict, features_types: Dict, pip_requirements: List[str], run_id):\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        with mlflow.start_run(run_id=run_id) as run:\n",
    "            test_preds = model.predict_proba(X_test)[:, 1]\n",
    "            test_log_loss = log_loss(y_test, test_preds)\n",
    "            train_preds = model.predict_proba(X_train)[:, 1]\n",
    "            train_log_loss = log_loss(y_train, train_preds)\n",
    "\n",
    "            mlflow.log_param(f\"{MODELS_NAME['MODEL1']}_version\", 1)\n",
    "            mlflow.log_param(f\"{MODELS_NAME['MODEL1']}_hyperparams\", hyperparams)\n",
    "            mlflow.log_metric(f\"{MODELS_NAME['MODEL1']}_test_log_loss\", test_log_loss)\n",
    "            mlflow.log_metric(f\"{MODELS_NAME['MODEL1']}_train_log_loss\", train_log_loss)\n",
    "\n",
    "            mlflow.log_param(f\"{MODELS_NAME['MODEL1']}_training_environment\", f\"https://{spark.conf.get('spark.databricks.workspaceUrl')}\")\n",
    "            mlflow.log_param(f\"{MODELS_NAME['MODEL1']}_training_timestamp\", datetime.now().isoformat())\n",
    "            end_time = time.time()\n",
    "            training_duration = timedelta(seconds=end_time - start_time)\n",
    "            mlflow.log_param(f\"{MODELS_NAME['MODEL1']}_training_duration\", str(training_duration))\n",
    "            mlflow.log_param(f\"{MODELS_NAME['MODEL1']}_training_run_id\", run.info.run_id)\n",
    "\n",
    "            features_types_str = {k: str(v) for k, v in features_types.items()}\n",
    "\n",
    "            mlflow.log_param(f\"{MODELS_NAME['MODEL1']}_features_types\", json.dumps(features_types_str))\n",
    "            mlflow.log_param(f\"{MODELS_NAME['MODEL1']}_encoding_scheme\", \"CategoricalDtype\")\n",
    "            mlflow.log_param(f\"{MODELS_NAME['MODEL1']}_scaling_strategy\", \"None\")\n",
    "\n",
    "            signature, input_example = create_robust_signature(X_test, model)\n",
    "            \n",
    "            model_info = mlflow.xgboost.log_model(\n",
    "                model, \n",
    "                artifact_path=\"xgboost-model\",\n",
    "                signature=signature,\n",
    "                input_example=input_example,\n",
    "                model_format=\"json\",\n",
    "                pip_requirements=pip_requirements\n",
    "            )\n",
    "\n",
    "            return run_id, model_info.model_uri\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in logging model and metrics: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X_train: pd.DataFrame, y_train: pd.Series, X_test: pd.DataFrame, y_test: pd.Series, hyperparams: Dict) -> XGBClassifier:\n",
    "    try:\n",
    "        model = XGBClassifier(**hyperparams)\n",
    "        model.fit(X_train, y_train, eval_set=[(X_test, y_test)])\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in model training: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_artifacts(model, encodings, features_types, run_id: str):\n",
    "    try:\n",
    "        with mlflow.start_run(run_id=run_id):  # Attach to the existing run\n",
    "            for artifact_name in [\"model\", \"encodings\", \"features_types\"]:\n",
    "                joblib.dump(eval(artifact_name), f\"./{artifact_name}.sav\", compress=3)\n",
    "                mlflow.log_artifact(f\"./{artifact_name}.sav\", \"prod_artifacts\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in saving artifacts: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(run_id):\n",
    "    try:\n",
    "        # Initialize Spark\n",
    "        spark = initialize_spark()\n",
    "        features_types = {\n",
    "                \"gender\": str, \n",
    "                \"campaign_id\": str, \n",
    "                \"traffic_partner_id\": str, \n",
    "                \"traffic_source_id\": float,  # Updated from Int64 to float64\n",
    "                \"os_name\": str, \n",
    "                \"age\": float,  # Updated from Int64 to float64\n",
    "                \"hour\": pd.Int64Dtype(), \n",
    "                \"household_income\": float,\n",
    "                \"position\": pd.Int64Dtype(),  # Retains Int64 as it doesn't have nulls\n",
    "            }\n",
    "        \n",
    "        features = list(features_types.keys())\n",
    "        categorical_features = [\n",
    "            \"gender\", \"campaign_id\", \"traffic_partner_id\", \"traffic_source_id\", \n",
    "            \"os_name\", \"hour\", \"position\",\n",
    "        ]\n",
    "\n",
    "        target = \"click\"\n",
    "        \n",
    "        # Load and preprocess data\n",
    "        logger.info(\"Loading data...\")\n",
    "        \n",
    "        df_spark_ = load_data(spark)\n",
    "        df_spark = clean_data(df_spark_)\n",
    "\n",
    "        # Get the current user's email to set the experiment name\n",
    "        user_email = get_current_user_email()\n",
    "        experiment_name = f\"/Users/{user_email}/{MODELS_NAME['MODEL1']}_{ENV_VARS['ENV']}_hyperparams\"\n",
    "        hyper_params_ = get_params_from_experiment(experiment_name)\n",
    "        \n",
    "        if hyper_params_ == None:  # if no experiment is available to fetch hyper-parameter values\n",
    "            hyperparams = {\n",
    "            'colsample_bytree': 0.6421218939603257,\n",
    "            'early_stopping_rounds': 9,\n",
    "            'learning_rate': 0.03711165197308055, \n",
    "            'max_depth': 8, \n",
    "            'n_estimators': 369,\n",
    "            'subsample': 0.9173736479523668,\n",
    "            'enable_categorical': True, \n",
    "            'eval_metric': 'logloss', \n",
    "            'tree_method': 'hist',\n",
    "            'max_cat_to_onehot': 1,\n",
    "            }\n",
    "        else:\n",
    "            hyperparams = {\n",
    "                'colsample_bytree': hyper_params_['colsample_bytree'],\n",
    "                'early_stopping_rounds': int(hyper_params_['early_stopping_rounds']),\n",
    "                'learning_rate': hyper_params_['learning_rate'], \n",
    "                'max_depth': int(hyper_params_['max_depth']),\n",
    "                'n_estimators': int(hyper_params_['n_estimators']),\n",
    "                'subsample': hyper_params_['subsample'],\n",
    "                'enable_categorical': True, \n",
    "                'eval_metric': 'logloss', \n",
    "                'tree_method': 'hist',\n",
    "                'max_cat_to_onehot': 1,\n",
    "            }\n",
    "        \n",
    "        print(\"hyperparams\", hyperparams)\n",
    "\n",
    "        # Add null values for robustness testing\n",
    "        null_samples = {\n",
    "            'campaign_id': df_spark.sample(False, 0.05).withColumn('campaign_id', F.lit(None)),\n",
    "            'traffic_source_id': df_spark.sample(False, 0.05).withColumn('traffic_source_id', F.lit(None)),\n",
    "            'traffic_partner_id': df_spark.sample(False, 0.05).withColumn('traffic_partner_id', F.lit(None))\n",
    "        }\n",
    "        df_spark = df_spark.unionAll(null_samples['campaign_id']) \\\n",
    "                        .unionAll(null_samples['traffic_source_id']) \\\n",
    "                        .unionAll(null_samples['traffic_partner_id'])\n",
    "\n",
    "        logger.info(\"Splitting data into train and test sets...\")\n",
    "        train_pd, test_pd = test_train_split(df_spark, train_test_ratio=0.8)\n",
    "\n",
    "        train_pd = apply_feature_types(train_pd, features_types)\n",
    "        test_pd = apply_feature_types(test_pd, features_types)\n",
    "\n",
    "        logger.info(f\"Train DataFrame dtypes after applying features_types:\\n{train_pd.dtypes}\")\n",
    "        logger.info(f\"Test DataFrame dtypes after applying features_types:\\n{test_pd.dtypes}\")\n",
    "\n",
    "        train_pd = convert_integers_with_missing_to_float(train_pd, features_types)\n",
    "        test_pd = convert_integers_with_missing_to_float(test_pd, features_types)\n",
    "\n",
    "        logger.info(f\"Train DataFrame dtypes after converting integers with missing values to float:\\n{train_pd.dtypes}\")\n",
    "        logger.info(f\"Train DataFrame dtypes after converting integers with missing values to float:\\n{test_pd.dtypes}\")\n",
    "        \n",
    "        encodings = get_encodings(train_pd, categorical_features)\n",
    "        train_pd = encode_features(train_pd, encodings)\n",
    "        test_pd = encode_features(test_pd, encodings)\n",
    "\n",
    "        X_train = train_pd[features]\n",
    "        y_train = train_pd[target]\n",
    "        X_test = test_pd[features]\n",
    "        y_test = test_pd[target]\n",
    "\n",
    "        # Train model\n",
    "        logger.info(\"Training the model...\")\n",
    "        model = train_model(X_train, y_train, X_test, y_test, hyperparams)\n",
    "\n",
    "        # Define pip requirements\n",
    "        pip_requirements = [\n",
    "            \"mlflow==2.11.3\", \"scikit-learn==1.3.0\", \"scipy==1.10.0\",\n",
    "            \"psutil==5.9.0\", \"pandas==1.5.3\", \"cloudpickle==2.2.1\",\n",
    "            \"numpy==1.23.5\", \"category-encoders==2.6.3\", \"xgboost==2.0.3\",\n",
    "            \"lz4==4.3.2\", \"typing-extensions==4.10.0\"\n",
    "        ]\n",
    "\n",
    "        # Log model and metrics\n",
    "        logger.info(\"Logging model and metrics...\")\n",
    "        run_id, model_uri = log_model_and_metrics(\n",
    "                                                    model=model, \n",
    "                                                    X_train=X_train, \n",
    "                                                    y_train=y_train, \n",
    "                                                    X_test=X_test, \n",
    "                                                    y_test=y_test, \n",
    "                                                    hyperparams=hyperparams, \n",
    "                                                    pip_requirements=pip_requirements, \n",
    "                                                    features_types=features_types,\n",
    "                                                    run_id=run_id  # Use the retrieved run_id\n",
    "                                                )\n",
    "\n",
    "        # Save artifacts\n",
    "        save_artifacts(model, encodings, features_types, run_id)\n",
    "\n",
    "        dbutils.jobs.taskValues.set(\"run_id\", run_id)\n",
    "        dbutils.jobs.taskValues.set(\"model_name\", MODELS_NAME['MODEL1'])\n",
    "        dbutils.jobs.taskValues.set(\"model_uri\", model_uri)\n",
    "\n",
    "        logger.info(\"Model training and logging completed successfully.\")\n",
    "        logger.info(f\"run_id received {run_id}\")\n",
    "        logger.info(f\"model_name received {MODELS_NAME['MODEL1']}\")\n",
    "        logger.info(f\"model_uri received {model_uri}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        error_message = f\"An error occurred in main execution: {str(e)}\\nTraceback: {traceback.format_exc()}\"\n",
    "        logger.error(error_message)\n",
    "        raise\n",
    "\n",
    "    logger.info(\"Script execution completed.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main(run_id=RUN_ID)\n",
    "    except Exception as e:\n",
    "        critical_error = f\"Critical error in main script execution: {str(e)}\\nTraceback: {traceback.format_exc()}\"\n",
    "        logger.critical(critical_error)\n",
    "        raise"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "dlt_pipeline",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
