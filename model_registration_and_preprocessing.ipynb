{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.pyfunc\n",
    "import mlflow.xgboost\n",
    "\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.functions import col, hour, to_timestamp, lit, udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.api.types import CategoricalDtype\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK, SparkTrials\n",
    "\n",
    "import joblib\n",
    "import logging\n",
    "import sys\n",
    "import json\n",
    "import traceback\n",
    "from typing import List, Dict, Any, Tuple\n",
    "import yaml\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "# Import libraries\n",
    "%run reference\n",
    "print(ENV_VARS, MODELS_NAME)\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_task_values():\n",
    "    try:\n",
    "        run_id = dbutils.jobs.taskValues.get(taskKey=\"model_retraining_task_1\", key=\"run_id\")\n",
    "        print(f\"Received run_id: {run_id}\")\n",
    "\n",
    "        model_uri = dbutils.jobs.taskValues.get(taskKey=\"model_retraining_task_1\", key=\"model_uri\")\n",
    "        print(f\"Received model_uri: {model_uri}\")\n",
    "\n",
    "        return run_id, model_uri\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error getting task values: {e}\")\n",
    "        raise\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_artifacts(run_id: str) -> Dict[str, str]:\n",
    "    try:\n",
    "        artifacts = {\n",
    "            'model': mlflow.artifacts.download_artifacts(run_id=run_id, artifact_path='prod_artifacts/model.sav'),\n",
    "            'features_types': mlflow.artifacts.download_artifacts(run_id=run_id, artifact_path='prod_artifacts/features_types.sav'),\n",
    "            'encodings': mlflow.artifacts.download_artifacts(run_id=run_id, artifact_path='prod_artifacts/encodings.sav')\n",
    "            # 'conda_env': mlflow.artifacts.download_artifacts(run_id=run_id, artifact_path='xgboost-model/conda.yaml')\n",
    "        }\n",
    "        return artifacts\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error downloading artifacts: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_conda_env_yaml(run_id: str) -> Dict[str, str]:\n",
    "    try:\n",
    "        return {\n",
    "            'conda_env': mlflow.artifacts.download_artifacts(run_id=run_id, artifact_path='xgboost-model/conda.yaml')\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error downloading conda env: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessingModelWrapper(mlflow.pyfunc.PythonModel):\n",
    "    def __init__(self, run_id: str):\n",
    "        try:\n",
    "            # run_id, model_name, model_uri = get_task_values()\n",
    "            # \n",
    "            self.RUN_ID = run_id\n",
    "            artifacts = download_artifacts(self.RUN_ID)            \n",
    "            self.model = joblib.load(artifacts['model'])\n",
    "            self.feature_types = joblib.load(artifacts['features_types'])\n",
    "            self.encodings = joblib.load(artifacts['encodings'])\n",
    "            print(\"Model, feature types, and encodings loaded successfully.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading model or preprocessors: {e}\")\n",
    "            raise e\n",
    "\n",
    "\n",
    "    def clean_os(self, os_name: str) -> str:\n",
    "        if pd.isna(os_name) or os_name in [\"null\", \"NA\", \"\", None]:\n",
    "            return \"\"\n",
    "        if os_name in [\"Android\", \"iOS\", \"Windows\", \"Mac\"]:\n",
    "            return os_name\n",
    "        return \"Other\"\n",
    "\n",
    "\n",
    "    def clean_gender(self, gender: str) -> str:\n",
    "        if pd.isna(gender) or gender in [\"null\", \"NA\", \"\", None]:\n",
    "            return \"\"\n",
    "        gender = gender.lower()\n",
    "        if gender in [\"f\", \"female\"]:\n",
    "            return \"F\"\n",
    "        if gender in [\"m\", \"male\"]:\n",
    "            return \"M\"\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "    def clean_age(self, age) -> float:\n",
    "        if age is None or age in [\"null\", \"NA\", \"\", None, -1]:\n",
    "            return np.nan\n",
    "        if isinstance(age, (int, float)):\n",
    "            return float(age)\n",
    "        if isinstance(age, str):\n",
    "            try:\n",
    "                return float(age)\n",
    "            except ValueError:\n",
    "                return np.nan\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "    def clean_household_income(self, household_income) -> float:\n",
    "        if household_income is None or household_income in [\"null\", \"NA\", \"\", None, -1]:\n",
    "            return np.nan\n",
    "        if isinstance(household_income, (int, float)):\n",
    "            return float(household_income)\n",
    "        if isinstance(household_income, str):\n",
    "            try:\n",
    "                return float(household_income)\n",
    "            except ValueError:\n",
    "                return np.nan\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "    def clean_traffic_source_id(self, traffic_source_id) -> float:\n",
    "        if traffic_source_id is None:\n",
    "            return np.nan\n",
    "        if isinstance(traffic_source_id, (int, float)):\n",
    "            return float(traffic_source_id)\n",
    "        if isinstance(traffic_source_id, str):\n",
    "            if traffic_source_id.isnumeric():\n",
    "                return float(traffic_source_id)\n",
    "            else:\n",
    "                return np.nan\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "    def clean_data(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        df[\"os_name\"] = df[\"os_name\"].apply(self.clean_os)\n",
    "        df[\"gender\"] = df[\"gender\"].apply(self.clean_gender)\n",
    "        df[\"age\"] = df[\"age\"].apply(self.clean_age)\n",
    "        df[\"household_income\"] = df[\"household_income\"].apply(self.clean_household_income)\n",
    "        df[\"traffic_source_id\"] = df[\"traffic_source_id\"].apply(self.clean_traffic_source_id)\n",
    "        \n",
    "        return df\n",
    "\n",
    "\n",
    "    def predict(self, context, model_input):\n",
    "        try:\n",
    "            # Handle input formats\n",
    "            if isinstance(model_input, dict) and 'instances' in model_input:\n",
    "                model_input = model_input['instances']\n",
    "\n",
    "            if isinstance(model_input, pd.DataFrame):\n",
    "                df = model_input\n",
    "            elif isinstance(model_input, list):\n",
    "                df = pd.DataFrame(model_input)\n",
    "            else:\n",
    "                return 'Invalid model input specified'\n",
    "            \n",
    "            # Check if the DataFrame is empty\n",
    "            # if df.empty:\n",
    "            #     return \"Input data is empty. Please provide valid input data.\"\n",
    "\n",
    "            # Clean the data\n",
    "            df = self.clean_data(df)\n",
    "\n",
    "            # Convert DataFrame to the correct types\n",
    "            df = df.astype(self.feature_types)\n",
    "            \n",
    "            original_campaign_id = df['campaign_id'].copy()\n",
    "\n",
    "            # Convert categorical features to the correct dtype\n",
    "            for feature in self.encodings:\n",
    "                df[feature] = df[feature].astype(CategoricalDtype(categories=self.encodings[feature]))\n",
    "            \n",
    "            # Generate predictions\n",
    "            preprocessed_input = df[self.feature_types.keys()]\n",
    "            pred_prob = self.model.predict_proba(preprocessed_input)\n",
    "\n",
    "            df['model_ctr'] = pred_prob[:, 1]\n",
    "            df['campaign_id'] = original_campaign_id\n",
    "            df['model_version'] = f'adflow_click_{MODELS_NAME[\"MODEL1\"]} - ' + str(self.RUN_ID)\n",
    "\n",
    "            specific_columns = ['campaign_id', 'position', 'model_ctr', \"model_version\"]\n",
    "            final = df[specific_columns]\n",
    "\n",
    "            return final\n",
    "\n",
    "        except Exception as e:\n",
    "            error_message = f\"Error processing input: {e}\"\n",
    "            logger.error(error_message)\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_and_log_model(run_id: str, model_name: str):\n",
    "    try:\n",
    "        with mlflow.start_run(run_id=run_id) as run:\n",
    "            # Create an instance of PreprocessingModelWrapper\n",
    "            model_wrapper = PreprocessingModelWrapper(run_id)\n",
    "\n",
    "            # Define pip requirements\n",
    "            pip_requirements = [\n",
    "                \"mlflow==2.11.3\", \"scikit-learn==1.3.0\", \"scipy==1.10.0\",\n",
    "                \"psutil==5.9.0\", \"pandas==1.5.3\", \"cloudpickle==2.2.1\",\n",
    "                \"numpy==1.23.5\", \"category-encoders==2.6.3\", \"xgboost==2.0.3\",\n",
    "                \"lz4==4.3.2\", \"typing-extensions==4.10.0\"\n",
    "            ]\n",
    "\n",
    "            # Log the model using mlflow.pyfunc\n",
    "            mlflow.pyfunc.log_model(\n",
    "                artifact_path=\"model\",\n",
    "                python_model=model_wrapper,\n",
    "                pip_requirements=pip_requirements\n",
    "            )\n",
    "\n",
    "            # Register the model\n",
    "            model_uri = f\"runs:/{run.info.run_id}/model\"\n",
    "            registered_model = mlflow.register_model(model_uri, model_name)\n",
    "            \n",
    "            print(f\"Model '{registered_model.name}' registered successfully with version '{registered_model.version}' with URI: {model_uri}\")\n",
    "\n",
    "            # Return the model name and version\n",
    "            return registered_model.name, registered_model.version\n",
    "\n",
    "    except mlflow.exceptions.MlflowException as e:\n",
    "        if \"Model with name\" in str(e):\n",
    "            print(f\"Model '{model_name}' already exists. Consider using a different name or version.\")\n",
    "        else:\n",
    "            logger.error(f\"Error registering model: {e}\")\n",
    "            raise e\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error during model registration: {e}\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    try:\n",
    "        # run_id, model_name, model_uri = get_task_values()\n",
    "        run_id, model_uri = get_task_values()\n",
    "        print(f\"Received run_id: {run_id}, model_name: {MODELS_NAME['MODEL1']}, model_uri: {model_uri}\")\n",
    "\n",
    "        # Register and log the model\n",
    "        registered_model_name, registered_model_version = register_and_log_model(run_id, MODELS_NAME['MODEL1'])\n",
    "        dbutils.jobs.taskValues.set(\"registered_model_name_1\", registered_model_name)\n",
    "        dbutils.jobs.taskValues.set(\"registered_model_version_1\", registered_model_version)\n",
    "        print(f\"Model '{registered_model_name}' registered successfully with version '{registered_model_version}' with URI: {model_uri}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during model registration: {e}\")\n",
    "        raise e\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "dlt_pipeline",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
