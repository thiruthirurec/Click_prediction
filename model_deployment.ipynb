{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "29b92224-42fb-4fb9-8146-de5c29992381",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.deployments import get_deploy_client\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "# Import libraries\n",
    "%run reference\n",
    "print(ENV_VARS, MODELS_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce8bd3b7-9d27-46fe-ba00-4e6190c57e39",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "MODEL_VERSION_1 = dbutils.jobs.taskValues.get(taskKey=\"register_model_task_1\", key=\"registered_model_version_1\")\n",
    "MODEL_VERSION_2 = dbutils.jobs.taskValues.get(taskKey=\"register_model_task_2\", key=\"registered_model_version_2\")\n",
    "\n",
    "databricks_host = spark.conf.get('spark.databricks.workspaceUrl')\n",
    "\n",
    "print(f\"Received registered_model_name_1: {MODELS_NAME['MODEL1']}\")\n",
    "print(f\"Received registered_model_version_1: {MODEL_VERSION_1}\")\n",
    "\n",
    "print(f\"Received registered_model_name_2: {MODELS_NAME['MODEL2']}\")\n",
    "print(f\"Received registered_model_version_2: {MODEL_VERSION_2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ec83f06-5d08-4ebb-a5b5-54d94dc852ec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_or_update_endpoint(model_name_1, model_version_1, model_name_2, model_version_2, endpoint_name):\n",
    "    try:\n",
    "        print(f\"Creating serving endpoint for {model_name_1}, {model_name_2}...\")\n",
    "        client = get_deploy_client(\"databricks\")\n",
    "\n",
    "        served_entities = [\n",
    "            {\n",
    "                \"entity_name\": model_name_1,\n",
    "                \"entity_version\": model_version_1,\n",
    "                \"workload_size\": ENV_VARS['WORKLOAD_SIZE'],\n",
    "                \"scale_to_zero_enabled\": True if ENV_VARS['SCALE_TO_ZERO_ENABLED'] == \"True\" else False\n",
    "            },\n",
    "            {\n",
    "                \"entity_name\": f\"{ENV_VARS['CATALOG']}.{ENV_VARS['SCHEMA']}.{model_name_2}\",\n",
    "                \"entity_version\": model_version_2,  \n",
    "                \"workload_size\": ENV_VARS['WORKLOAD_SIZE'],\n",
    "                \"scale_to_zero_enabled\": True if ENV_VARS['SCALE_TO_ZERO_ENABLED'] == \"True\" else False\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        auto_capture_config = {\n",
    "            \"catalog_name\": ENV_VARS['CATALOG'],\n",
    "            \"schema_name\": \"click_api\",\n",
    "            \"table_name_prefix\": endpoint_name,\n",
    "            \"enabled\": True                         \n",
    "        }\n",
    "\n",
    "        traffic_config = {\n",
    "            \"routes\": [\n",
    "                {\n",
    "                    \"served_model_name\": f\"{model_name_1}-{model_version_1}\",\n",
    "                    \"traffic_percentage\": ENV_VARS['MODEL1_TRAFFIC_PERCENTAGE']\n",
    "                },\n",
    "                {\n",
    "                    \"served_model_name\": f\"{model_name_2}-{model_version_2}\",\n",
    "                    \"traffic_percentage\": ENV_VARS['MODEL2_TRAFFIC_PERCENTAGE']\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        # First try updating the existing endpoint.\n",
    "        endpoint = client.update_endpoint(\n",
    "            endpoint=endpoint_name,\n",
    "            config={\n",
    "                \"served_entities\": served_entities,\n",
    "                \"auto_capture_config\": auto_capture_config,\n",
    "                \"traffic_config\": traffic_config\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # assert endpoint[\"name\"] == model_name_1\n",
    "        # assert endpoint[\"state\"][\"config_update\"] == \"IN_PROGRESS\"\n",
    "        print (f\"Serving endpoint {endpoint_name} updated successfully using version {model_version_1}, {model_version_2} of ML model [{model_name_1}, {model_name_2}]\")\n",
    "\n",
    "    except Exception as e:\n",
    "        \n",
    "        if hasattr(e, \"response\") and hasattr(e.response, \"json\"):\n",
    "            error = e.response.json()\n",
    "            error_code = error.get(\"error_code\")\n",
    "            \n",
    "            if error_code:\n",
    "                # If it doesn't already exist, create a new endpoint.\n",
    "                # if error_code == 'RESOURCE_DOES_NOT_EXIST':\n",
    "                catalog_name = ENV_VARS['CATALOG']\n",
    "                database_name = \"click_api\"\n",
    "\n",
    "                create_database_sql = f\"CREATE SCHEMA IF NOT EXISTS {catalog_name}.{database_name}\"\n",
    "                spark.sql(create_database_sql)\n",
    "                \n",
    "                endpoint = client.create_endpoint(\n",
    "                    name=endpoint_name,\n",
    "                    config={\n",
    "                        \"served_entities\": served_entities,\n",
    "                        \"auto_capture_config\": auto_capture_config,\n",
    "                        \"traffic_config\": traffic_config\n",
    "                    }\n",
    "                )\n",
    "                \n",
    "                # Ensure that the endpoint is being created using the registered model.\n",
    "                assert endpoint[\"pending_config\"][\"served_entities\"][0][\"name\"] == f\"{model_name_1}-{model_version_1}\"\n",
    "                assert endpoint[\"pending_config\"][\"served_entities\"][1][\"name\"] == f\"{model_name_2}-{model_version_2}\"\n",
    "                print (f\"Serving endpoint {endpoint_name} made successfuly for combined ML Models {model_name_1}-{model_version_1}, {model_name_2}-{model_version_2}\")\n",
    "                \n",
    "                # elif error_code == \"RESOURCE_CONFLICT\":\n",
    "                #     print(f\"Endpoint {endpoint_inside} is currently being updated. Please try again later\")\n",
    "        else:\n",
    "            print(\"ERROR: \" + e)\n",
    "            raise e\n",
    "\n",
    "\n",
    "create_or_update_endpoint(MODELS_NAME['MODEL1'], MODEL_VERSION_1, MODELS_NAME['MODEL2'], MODEL_VERSION_2, ENV_VARS['ENDPOINT_NAME'])\n",
    "\n",
    "# Construct the model URL\n",
    "model_url = f\"https://{databricks_host}/serving-endpoints/{ENV_VARS['ENDPOINT_NAME']}/invocations\"\n",
    "print(model_url)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "model_deployment",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
